\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{array}
\usepackage{fancyhdr}
\usepackage{amssymb}
\usepackage[shortlabels]{enumitem}
\usepackage{amsthm}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{hyperref}

\DeclareMathOperator{\R}{\mathbb R}
\newcommand{\mb}{\mathbf}
\newcommand{\dotp}{\boldsymbol{\cdot}}
\DeclareMathOperator{\uu}{\mathbf u}
\DeclareMathOperator{\vv}{\mathbf v}
\DeclareMathOperator{\g}{\mathbf g}
\DeclareMathOperator{\x}{\mathbf x}
\DeclareMathOperator{\y}{\mathbf y}
\DeclareMathOperator{\z}{\mathbf z}
\def\epsilon{\varepsilon}

\pagestyle{fancy}
\fancyhead[L]{Banghao Chi}
\fancyhead[C]{Homework 9}
\fancyhead[R]{18th Apr}

\fancyfoot[C]{\thepage}

\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0.5pt}

\begin{document}

\section*{Exercise 1}
(28 points) Use the gradient form of the KKT theorem to solve the following problems. (In some cases, you may have to change the form of the problem before using the theorem.)
\begin{itemize}
\item \begin{equation*}
(P)\qquad\left\{\begin{aligned}
& \underset{  (x,y) \in \R^2}{\text{minimize}}
& & x^2 + 2y^2 \\
& \text{subject to} && x-y+1 \leq 0 \\
\end{aligned}\right.
\end{equation*}
\item \begin{equation*}
(P)\qquad\left\{\begin{aligned}
& \underset{  (x,y) \in \R^2}{\text{minimize}}
& & x+y \\
& \text{subject to} && \phantom{xxxxx} x^2 - y \leq 0 \\
& && -2x-y+8 \leq 0
\end{aligned}\right.
\end{equation*}
\item Find the vector $(x_1, \dots, x_n)$ that maximizes $x_1 + \dots + x_n$ subject to $x_1^q + \dots + x_n^q = 1$, where $q \geq2$ is even.
\item Let $\mb a \in \R^n$ satisfy $\|\mb a\| \geq 1$. Show that the closest vector to $\mb a$ in the closed unit ball $\{\mb z \in \R^n: \|\mb z \| \leq 1\}$ is $\frac{\mb a}{\|\mb a\|}$.
\end{itemize}

\textbf{Solution:} \\

\begin{enumerate}

\item
\[
\begin{aligned}
  &\min_{(x,y)\in\mathbb R^{2}} && f(x,y)=x^{2}+2y^{2}\\
  &\text{s.t.}                 && g(x,y)=x-y+1\le 0 .
\end{aligned}
\]

Because $f$ is convex and $g$ is affine, Slater’s condition holds and the KKT
conditions are necessary and sufficient.  
Introduce a multiplier $\lambda\ge 0$ and form the Lagrangian
$L(x,y,\lambda)=x^{2}+2y^{2}+\lambda(x-y+1)$.

\[
\nabla_{x,y}L=
\begin{pmatrix}
2x+\lambda\\[2pt]
4y-\lambda
\end{pmatrix}=0
\quad\Longrightarrow\quad
x=-\frac{\lambda}{2},\qquad
y=\frac{\lambda}{4}.
\]

Complementary slackness gives $\lambda(x-y+1)=0$.
The point $(0,0)$ (obtained when $\lambda=0$) violates $x-y+1\le0$,  
so $\lambda>0$ and the constraint is active:

\[
x-y+1=0\;\Longrightarrow\;
-\frac{\lambda}{2}-\frac{\lambda}{4}+1=0
\;\Longrightarrow\;
\lambda=\frac43 .
\]

Hence
\[
x^{*}=-\frac23,\qquad
y^{*}= \frac13,\qquad
f^{*}=x^{*2}+2y^{*2}= \frac23 .
\]

\item
\[
\begin{aligned}
  &\min_{(x,y)\in\mathbb R^{2}} && f(x,y)=x+y\\
  &\text{s.t.}                 && g_{1}(x,y)=x^{2}-y\le 0,\\
  &                            && g_{2}(x,y)=-2x-y+8\le 0 .
\end{aligned}
\]

Let multipliers $\lambda_{1},\lambda_{2}\ge 0$.
Stationarity:

\[
\nabla f+\lambda_{1}\nabla g_{1}+\lambda_{2}\nabla g_{2}=0
\Longrightarrow
\begin{cases}
1+2\lambda_{1}x-2\lambda_{2}=0,\\
1-\lambda_{1}-\lambda_{2}=0.
\end{cases}
\tag{S}
\]

\paragraph{Case $\lambda_{1}>0$.}
Then $g_{1}=0$ so $y=x^{2}$.  
From (S) we obtain $\lambda_{2}=1-\lambda_{1}$ and
$2\lambda_{1}(x+1)=1$.  If $\lambda_{2}>0$ the second constraint is
active:

\[
x^{2}+2x-8=0 \quad\Longrightarrow\quad x=2\ (\text{or }x=-4).
\]

Only $x=2$ satisfies $2\lambda_{1}(x+1)=1$ (it gives $\lambda_{1}=1/6$,
$\lambda_{2}=5/6>0$).  
Thus $(x^{*},y^{*})=(2,4)$ with objective value $f^{*}=6$.

\paragraph{All other cases.}
Either $\lambda_{1}=0$ (impossible by stationarity) or
$\lambda_{1}>0$ but $\lambda_{2}=0$ (gives an infeasible point).  
Hence the unique KKT point $(2,4)$ is the global minimizer.

\item
\[
\max_{\mathbf x\in\mathbb R^{n}}\;
\Bigl(\sum_{i=1}^{n}x_{i}\Bigr)
\quad\text{s.t.}\quad
\sum_{i=1}^{n}x_{i}^{\,q}=1,
\qquad q\ge 2\text{ even}.
\]

The feasible set is compact, the constraint is smooth, and Slater holds.
Introduce a multiplier $\lambda$ (no sign constraint: the equality
is moved to the RHS of the Lagrangian).

\[
L(\mathbf x,\lambda)=
\sum_{i=1}^{n}x_{i}-\lambda\Bigl(\sum_{i=1}^{n}x_{i}^{\,q}-1\Bigr),
\quad
\partial_{x_{i}}L=1-\lambda qx_{i}^{\,q-1}=0.
\]

Thus every $x_{i}$ is identical:

\[
x_{1}=x_{2}=\dots=x_{n}=c
\quad\Longrightarrow\quad
nc^{\,q}=1\;\Longrightarrow\;c=n^{-1/q}.
\]

The multiplier is $\lambda=\dfrac{n^{\,(q-1)/q}}{q}$ and

\[
x_{i}^{*}=n^{-1/q}\;(i=1,\dots,n),\qquad
\max\; \sum x_{i}=n^{\,1-1/q} .
\]

(The value coincides with Hölder’s inequality upper bound, confirming
optimality.)

\item
\[
\min_{\mathbf z\in\mathbb R^{n}}
\frac12\|\mathbf z-\mathbf a\|^{2}
\quad\text{s.t.}\quad
g(\mathbf z)=\|\mathbf z\|^{2}-1\le 0,
\qquad\|\mathbf a\|\ge 1.
\]

Let $\lambda\ge 0$.  
Stationarity:

\[
\mathbf z-\mathbf a+2\lambda\mathbf z=\mathbf 0
\Longrightarrow
(1+2\lambda)\mathbf z=\mathbf a
\quad\Longrightarrow\quad
\mathbf z=\frac{\mathbf a}{1+2\lambda}.
\]

\emph{Case $\lambda=0$.}  
Then $\mathbf z=\mathbf a$ is feasible only if $\|\mathbf a\|\le 1$.  
Here $\|\mathbf a\|\ge1$, so $\lambda>0$.

\emph{Case $\lambda>0$ (constraint active).}  
Complementary slackness forces $g(\mathbf z)=0$, i.e. $\|\mathbf z\|=1$:

\[
\bigl\|{\mathbf a}/{(1+2\lambda)}\bigr\|=1
\;\Longrightarrow\;
1+2\lambda=\|\mathbf a\|.
\]

Hence $\lambda=(\|\mathbf a\|-1)/2$ and

\[
\mathbf z^{*}=\dfrac{\mathbf a}{\|\mathbf a\|}.
\]

Therefore, the point $\mathbf a/\|\mathbf a\|$ is the unique minimiser,
i.e.\ the closest point to $\mathbf a$ on (or in) the closed unit ball.
\end{enumerate}


\newpage

\section*{Exercise 2}
(12 points) Consider the convex program 
 \begin{equation*}
(P)\qquad\left\{\begin{aligned}
& \underset{  (x,y) \in \R^2}{\text{minimize}}
&  x^2 + y^2 \\
& \text{subject to} & 1-y \leq 0 \\
 && -2x-y +2 \leq 0 \\
 && x^2 + y - 9 \leq 0 \\
 && x^2 - y - 9 \leq 0
\end{aligned}\right.
\end{equation*}
\begin{itemize}
\item Sketch the feasibility region, and use this sketch to find the solution $(x^*,y^*)$ to $P$.
\item Let $(\lambda_1^*,\dots,\lambda_4^*)$ be a sensitivity vector of $P$. By considering the feasibility region and the objective function, explain which of $\lambda_1^*, \dots, \lambda_4^*$ should be zero and which should be positive.
\item Using the gradient form of the KKT theorem along with the fact that you know the solution to $P$, find a sensitivity vector of $P$.
\item Write the Lagrangian $L(x,y,\lambda_1, \lambda_2,\lambda_3,\lambda_4)$ of $P$.
\item Show that for all $(x,y) \in \R^2$ and nonnnegative $(\lambda_1,\dots,\lambda_4)$,
\[L(x^*,y^*,\lambda_1,\lambda_2,\lambda_3,\lambda_4) \leq L(x^*,y^*,\lambda_1^*,\lambda_2^*,\lambda_3^*,\lambda_4^*) \leq L(x,y,\lambda_1^*,\lambda_2^*,\lambda_3^*,\lambda_4^*).\]
\end{itemize}

\textbf{Solution:} \\

\begin{enumerate}
\item  Feasible region  
      The four constraints can be rewritten as  
      \[
      \begin{aligned}
      &y\ge 1, \qquad\qquad\qquad
      &&y\ge 2-2x,\\
      &y\le 9-x^{2},\qquad
      &&y\ge x^{2}-9.
      \end{aligned}
      \] 
      The last two inequalities describe the vertical strip between the two parabolas
      \(y=9-x^{2}\) (opening downward) and \(y=x^{2}-9\) (opening upward), intersecting at
      \((\pm 3,0)\).
      Intersecting this strip with the half–planes \(y\ge 1\) and
      \(y\ge 2-2x\) leaves a convex, bounded set whose lowest point lies on
      the line \(y=1\).  
      On that line feasibility requires \(2x+1\ge 2\), i.e.\ \(x\ge \tfrac12\).
      The objective \(f(x,y)=x^{2}+y^{2}\) restricted to the boundary segment
      \((x,1)\;(x\ge\tfrac12)\) is \(x^{2}+1\), increasing in \(x\).
      Hence the minimiser is  
      \[
      (x^{*},y^{*})=\left(\tfrac12,\,1\right), \qquad f^{*}=1+\Bigl(\tfrac12\Bigr)^{2}= \tfrac54 .
      \]

\item  Active constraints and signs of the multipliers  
      At \((x^{*},y^{*})\) the first two constraints are tight:
      \(y^{*}-1=0\) and \(-2x^{*}-y^{*}+2=0\).
      The two quadratic constraints are slack:
      \(x^{*\,2}+y^{*}-9=-7.75<0\) and \(x^{*\,2}-y^{*}-9=-9.75<0\).
      Complementary slackness therefore forces  
      \[
      \lambda_{1}^{*}>0,\quad\lambda_{2}^{*}>0,\quad
      \lambda_{3}^{*}=0,\quad\lambda_{4}^{*}=0 .
      \]

\item  Sensitivity vector via the gradient form of KKT  
      Let 
      \[
      g_{1}=1-y,\;
      g_{2}=-2x-y+2,\;
      g_{3}=x^{2}+y-9,\;
      g_{4}=x^{2}-y-9 .
      \]
      Stationarity requires  
      \[
      \nabla f(x^{*},y^{*})+\sum_{i=1}^{4}\lambda_{i}^{*}\nabla g_{i}(x^{*},y^{*})=0 .
      \]
      Using \(\nabla f(1/2,1)=(1,2)\) and  
      \[
      \nabla g_{1}=(0,-1),\;
      \nabla g_{2}=(-2,-1),\;
      \nabla g_{3}=(1,1),\;
      \nabla g_{4}=(1,-1)
      \]
      together with \(\lambda_{3}^{*}=\lambda_{4}^{*}=0\) gives the linear system
      \[
      \begin{aligned}
      1-2\lambda_{2}^{*} &=0,\\
      2-\lambda_{1}^{*}-\lambda_{2}^{*} &=0 .
      \end{aligned}
      \]
      Solving yields  
      \[
      (\lambda_{1}^{*},\lambda_{2}^{*},\lambda_{3}^{*},\lambda_{4}^{*})
      =\bigl(\tfrac32,\;\tfrac12,\;0,\;0\bigr).
      \]

\item  Lagrangian  
      \[
      L(x,y,\lambda_{1},\lambda_{2},\lambda_{3},\lambda_{4})
      =x^{2}+y^{2}
      +\lambda_{1}(1-y)
      +\lambda_{2}(-2x-y+2)
      +\lambda_{3}(x^{2}+y-9)
      +\lambda_{4}(x^{2}-y-9).
      \]

\item  Saddle‑point (min–max) property  
      For any \((x,y)\in\mathbb R^{2}\) and any multipliers \(\lambda_{i}\ge 0\),
      primal feasibility of \((x^{*},y^{*})\) gives \(g_{i}(x^{*},y^{*})\le 0\).
      Hence, fixing \((\lambda_{i})\),
      \[
      L(x^{*},y^{*},\lambda_{1},\dots,\lambda_{4})
      =f^{*}+\sum_{i}\lambda_{i}g_{i}(x^{*},y^{*})
      \le f^{*}=L(x^{*},y^{*},\lambda_{1}^{*},\dots,\lambda_{4}^{*}),
      \]
      because the active constraints have the largest non‑positive
      coefficients \(g_{i}(x^{*},y^{*})\).
      Conversely, for any \((x,y)\),
      weak duality gives  
      \[
      L(x,y,\lambda_{1}^{*},\dots,\lambda_{4}^{*})
      \ge f(x^{*},y^{*}) = L(x^{*},y^{*},\lambda_{1}^{*},\dots,\lambda_{4}^{*}),
      \]
      because \((\lambda_{i}^{*})\) is dual‑optimal.  
      Therefore  
      \[
      L(x^{*},y^{*},\lambda_{1},\lambda_{2},\lambda_{3},\lambda_{4})
      \le
      L(x^{*},y^{*},\lambda_{1}^{*},\lambda_{2}^{*},\lambda_{3}^{*},\lambda_{4}^{*})
      \le
      L(x,y,\lambda_{1}^{*},\lambda_{2}^{*},\lambda_{3}^{*},\lambda_{4}^{*}),
      \]
      establishing the required inequality.
\end{enumerate}


\newpage

\section*{Exercise 3 (Extra Credit)}
(10 points, extra credit)
This question comes from a set of job interview questions for a data science position. It can be solved using ideas from Chapter 4.

Let $\mb a, \mb b, \mb w$ be vectors of length $n$ with positive entries. Design an algorithm that prints the floating point vector $\mb x = (x_1, \dots, x_n) \in \R^n$ maximizing
\[\sum_{i=1}^n ( a_i x_i - w_i x_i^2 )\]
subject to $\mb b \dotp \mb x = 0$, and subject to the following constraints:
\begin{itemize}
\item $\mb a$, $\mb b$, and $\mb w$ are stored in read-only memory.
\item You can print values that you have stored in memory, but you cannot access printed values if the memory from which they have been printed has been overwritten.
\item You are only allowed to store $O(1)$ bits of memory. (You have enough memory to store constantly many floating point numbers, but you don't have enough memory to store an $n$-length vector.)
\end{itemize}
To give you a better idea of the question's constraints, consider the following. If you want to consider a sum like $\mb b + \mb w$, you cannot store the sum vector in memory, because it requires $\Omega(n)  \gg O(1)$ bits. However, you can print the vector $\mb b + \mb w$ as follows. For $i=1$ to $n$, copy $b_i$ and $w_i$ to memory, store the sum $b_i + w_i$ in a third area of memory, print the sum, and then delete all stored values. This process will print all coordinates of $\mb b + \mb w$, one by one. Since you only need to store three values at a time, this process takes $O(1)$ bits of memory. However, after the process is done, you cannot look up values of the sum $\mb b + \mb w$, and if you want to know what they are, you need to compute them again.

If you want to use operations involving matrices with $n$ rows or $n$ columns, you need to be very careful about how you do these operations, because such a matrix cannot be stored in memory. Also, if you want to solve a system of equations involving many variables, you need to be careful about how many variables you are storing at a time. If your system has $\Omega(n)$ variables, then that is too many to store in memory at once. \\

\textbf{Solution:} \\

Skipping this question.

\end{document}