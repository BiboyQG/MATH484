\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{array}
\usepackage{fancyhdr}
\usepackage{amssymb}
\usepackage[shortlabels]{enumitem}

\DeclareMathOperator{\R}{\mathbb R}

\pagestyle{fancy}
\fancyhead[L]{Banghao Chi}
\fancyhead[C]{Homework 3}
\fancyhead[R]{11th Feb}

\fancyfoot[C]{\thepage}

\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0.5pt}

\begin{document}

\section*{Exercise 1}
(10 points) Suppose that a function $f: \mathbb{R}^n \to \mathbb{R}$ satisfies the property \\

($\star$) For each pair $\mathbf{x}, \mathbf{y} \in \mathbb{R}^n$ and value $t \in [0,1]$, $f(t\mathbf{x} + (1-t)\mathbf{y}) \leq tf(\mathbf{x}) + (1-t)f(\mathbf{y})$ (In other words, $f$ is convex). \\

Prove that for all values $k \geq 1$, values $\lambda_1,\ldots,\lambda_k \geq 0$ satisfying $\lambda_1 + \cdots + \lambda_k = 1$, and $\mathbf{x}_1,\ldots,\mathbf{x}_k \in \mathbb{R}^n$,

$$f(\lambda_1\mathbf{x}_1 + \cdots + \lambda_k\mathbf{x}_k) \leq \lambda_1f(\mathbf{x}_1) + \cdots + \lambda_kf(\mathbf{x}_k).$$

In your proof, do not use any properties of $f$ except for the property ($\star$). \\

\textit{What I really want to say is this: Do not write "Since $f$ is convex, Jensen's inequality implies the result."} \\

\textbf{Solution: } \\

We prove this by induction on $k$. \\

Base case ($k=1$): 
When $k=1$, we have $\lambda_1 = 1$ (since $\lambda_1 + \cdots + \lambda_k = 1$). \\
Then $f(\lambda_1\mathbf{x}_1) = f(\mathbf{x}_1) = \lambda_1f(\mathbf{x}_1)$, so the inequality holds. \\

Base case ($k=2$): 
This is exactly property ($\star$) with $t = \lambda_1$ and $1-t = \lambda_2$, so the inequality holds. \\

Inductive step: 
Assume the statement holds for some $k \geq 2$. We'll prove it for $k+1$. \\

Let $\lambda_1,\ldots,\lambda_{k+1} \geq 0$ with $\sum_{i=1}^{k+1} \lambda_i = 1$, and let $\mathbf{x}_1,\ldots,\mathbf{x}_{k+1} \in \mathbb{R}^n$. \\

If any $\lambda_i = 0$, we can remove that term and apply the inductive hypothesis. So assume all $\lambda_i > 0$ for now. \\

Let $s = \lambda_1 + \cdots + \lambda_k$. Note that $s + \lambda_{k+1} = 1$, $1 = \frac{\lambda_1}{s} + \cdots + \frac{\lambda_{k}}{s}$. \\

Define $\mathbf{y} = \frac{1}{s}(\lambda_1\mathbf{x}_1 + \cdots + \lambda_k\mathbf{x}_k)$ \\

Then $\lambda_1\mathbf{x}_1 + \cdots + \lambda_{k+1}\mathbf{x}_{k+1}$ 
$= s\mathbf{y} + \lambda_{k+1}\mathbf{x}_{k+1}$
$= s\mathbf{y} + (1-s)\mathbf{x}_{k+1}$ \\

By property ($\star$) with $t = s$:
$$f(s\mathbf{y} + (1-s)\mathbf{x}_{k+1}) \leq sf(\mathbf{y}) + (1-s)f(\mathbf{x}_{k+1})$$

By the inductive hypothesis applied to $\mathbf{x}_1,\ldots,\mathbf{x}_k$ with coefficients $\frac{\lambda_1}{s},\ldots,\frac{\lambda_k}{s}$ (which sum to 1):
$$f(\mathbf{y}) = f(\frac{1}{s}(\lambda_1\mathbf{x}_1 + \cdots + \lambda_k\mathbf{x}_k)) \leq \frac{\lambda_1}{s}f(\mathbf{x}_1) + \cdots + \frac{\lambda_k}{s}f(\mathbf{x}_k)$$

Therefore:
\begin{align*}
f(\lambda_1\mathbf{x}_1 + \cdots + \lambda_{k+1}\mathbf{x}_{k+1}) &= f(s\mathbf{y} + (1-s)\mathbf{x}_{k+1}) \\
&\leq sf(\mathbf{y}) + (1-s)f(\mathbf{x}_{k+1}) \\
&\leq s(\frac{\lambda_1}{s}f(\mathbf{x}_1) + \cdots + \frac{\lambda_k}{s}f(\mathbf{x}_k)) + \lambda_{k+1}f(\mathbf{x}_{k+1}) \\
&= \lambda_1f(\mathbf{x}_1) + \cdots + \lambda_{k+1}f(\mathbf{x}_{k+1})
\end{align*}

By the principle of mathematical induction, the statement holds for all $k \geq 1$.

\newpage

\section*{Exercise 2}
(10 points) Let $\mathbf{x} \in \mathbb{R}^n$ and $r > 0$. Let $\|\cdot\|: \mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}$ be a norm function given by some arbitrary inner product. Fix a vector $\mathbf{x}_0 \in \mathbb{R}^n$ and a real number $r > 0$. Prove that the set

$$S = \{\mathbf{x} \in \mathbb{R}^n : \|\mathbf{x} - \mathbf{x}_0\| < r\}$$
is convex.

\textbf{Solution: } \\



\newpage

\section*{Exercise 3}
(8 points) For each of the following matrices, determine whether it is positive semidefinite, negative semidefinite, or indefinite. If it is also positive definite or negative definite, state this.
\begin{itemize}
\item $\begin{bmatrix} 4 & 1 \\ 1 & -2 \end{bmatrix}$

\item $\begin{bmatrix} -12 & 1 & 3 \\ 1 & -1 & 0 \\ 3 & 0 & -2 \end{bmatrix}$

\item $\begin{bmatrix} 5 & -1 & -2 \\ -1 & 1 & 0 \\ -2 & 0 & 1 \end{bmatrix}$
\end{itemize}

\textbf{Solution: } \\



\newpage

\section*{Exercise 4}
(12 points)

(a) Compute the eigenvalues of the matrix $B = \begin{bmatrix} 2 & 5 \\ 5 & 2 \end{bmatrix}$.

(b) Consider the matrix $A = \begin{bmatrix} 2 & 7 & 13 & 5 \\ 7 & -4 & \frac{2}{5} & 16 \\ 13 & \frac{2}{5} & \frac{1}{2} & 11 \\ 5 & 16 & 11 & 2 \end{bmatrix}$. Find unit vectors $\mathbf{u}, \mathbf{v} \in \mathbb{R}^4$ (that is, $\|\mathbf{u}\| = \|\mathbf{v}\| = 1$) satisfying $\mathbf{u}^T A\mathbf{u} = 7$ and $\mathbf{v}^T A\mathbf{v} = -3$. Justify why your choices for $\mathbf{u}$ and $\mathbf{v}$ are valid.
(\textit{Hint: Use (a).}) \\

(c) Let $f: \mathbb{R}^4 \to \mathbb{R}$ be a function, and let $\mathbf{x}^* \in \mathbb{R}^4$. Suppose that $\nabla f(\mathbf{x}^*) = 0$ and

$$Hf(\mathbf{x}^*) = A = \begin{bmatrix} 2 & 7 & 13 & 5 \\ 7 & -4 & \frac{2}{5} & 16 \\ 13 & \frac{2}{5} & \frac{1}{2} & 11 \\ 5 & 16 & 11 & 2 \end{bmatrix}.$$

Let $$\phi_1(t) = f(\mathbf{x}^* + t\mathbf{u})$$ and $$\phi_2(t) = f(\mathbf{x}^* + t\mathbf{v})$$ be single-variable functions, where $\mathbf{u}$ and $\mathbf{v}$ are taken from part (b). Show that $\phi_1$ has a strict local minimizer at $t = 0$, while $\phi_2$ has a strict local maximizer at $t = 0$.

\textbf{Solution: } \\



\end{document}